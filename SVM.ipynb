# 第一部分：导入所有需要的库
import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

# 第二部分：定义预处理函数 preprocess_text
def preprocess_text(text):
    """
    对单条英文评论进行预处理
    步骤：1. 文本清洗 -> 2. 分词 -> 3. 词形还原 -> 4. 去除停用词
    """
    # 1. 文本清洗：移除URL、HTML标签、非字母字符，并转换为小写
    text = re.sub(r'http\S+', '', text)  # 移除URL
    text = re.sub(r'<[^>]+>', '', text)   # 移除HTML标签
    text = re.sub(r'[^a-zA-Z\s]', '', text) # 移除非字母字符，保留空格
    text = text.lower().strip() # 转换为小写并去除首尾空格
    
    # 2. 分词：将句子分割成单词列表
    words = word_tokenize(text)
    
    # 3. 词形还原 + 4. 去除停用词
    filtered_words = []
    for word in words:
        lemma_word = lemmatizer.lemmatize(word) # 进行词形还原
        # 只保留不在停用词列表中且长度大于1的单词（过滤掉单字母）
        if lemma_word not in stop_words and len(lemma_word) > 1:
            filtered_words.append(lemma_word)
            
    # 用空格将处理后的单词列表重新组合成字符串，便于后续TF-IDF处理
    return ' '.join(filtered_words)

# 第三部分：主程序流程
# 1. 下载NLTK数据包
print("正在下载NLTK数据包...")
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
print("NLTK数据包下载完成！\n")

# 2. 初始化工具
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

positive_reviews = []
positive_labels = []

try:
    with open('sample.positive.txt', 'r', encoding='utf-8') as file:
        content = file.read()
        # 使用正则表达式找到所有<review>标签对之间的内容
        # re.DOTALL 标志使得 . 也能匹配换行符
        pattern = r'<review id="\d+">(.*?)</review>'
        matches = re.findall(pattern, content, re.DOTALL)
        
        for match in matches:
            # 匹配到的内容可能包含多行和首尾空白，进行清理
            cleaned_review = re.sub(r'\s+', ' ', match).strip()  # 将多个空白符（包括换行）替换为一个空格
            if cleaned_review:
                positive_reviews.append(cleaned_review)
                positive_labels.append(1)

    positive_df = pd.DataFrame({'review': positive_reviews, 'label': positive_labels})
    print(f"积极评论读取完成！解析XML标签后，有效评论为: {len(positive_df)} 条。")

except Exception as e:
    print("读取积极评论文件时失败，错误信息:", e)

negative_reviews = []
negative_labels = []

try:
    with open('sample.negative.txt', 'rb') as file:
        for line in file:
            decoded_line = line.decode('utf-8', errors='ignore').strip()
            if decoded_line:
                negative_reviews.append(decoded_line)
                negative_labels.append(0)

    negative_df = pd.DataFrame({'review': negative_reviews, 'label': negative_labels})
    print("消极评论读取成功！读取了", len(negative_df), "条评论。")
except Exception as e:
    print("读取消极评论文件时失败，错误信息:", e)

# 4. 创建combined_df
combined_df = pd.concat([positive_df, negative_df], ignore_index=True)
combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)

print("\n数据合并完成！")
print(f"总评论数: {len(combined_df)}")
print("标签分布:")
print(combined_df['label'].value_counts())

# 5. 应用预处理函数
print("\n正在预处理训练集评论...")
combined_df['processed_review'] = combined_df['review'].apply(preprocess_text)
print("预处理完成！")

# 6. 查看结果
print("\n预处理效果对比：")
sample_index = 0
print("原始评论：", combined_df.loc[sample_index, 'review'])
print("处理后评论：", combined_df.loc[sample_index, 'processed_review'])
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# 划分训练集和测试集
X = combined_df['processed_review']  # 特征：处理后的文本
y = combined_df['label']              # 标签：情感类别

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"训练集样本数: {len(X_train)}")
print(f"测试集样本数: {len(X_test)}")

# 初始化TF-IDF向量化器
tfidf_vectorizer = TfidfVectorizer(
    max_features=5000,    # 保留最重要的5000个特征词
    ngram_range=(1, 2)     # 同时考虑单词和双词组合
)

# 对训练集进行拟合和转换
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

# 对测试集只进行转换（重要：避免数据泄露！）
X_test_tfidf = tfidf_vectorizer.transform(X_test)

print("特征提取完成！")
print(f"训练集特征维度: {X_train_tfidf.shape}")
print(f"测试集特征维度: {X_test_tfidf.shape}")
from sklearn.svm import SVC

# 初始化SVM模型
svm_model = SVC(
    kernel='linear',      # 线性核函数，适合文本数据
    random_state=42,       # 设置随机种子确保结果可复现
    probability=True      # 需要计算概率以便后续评估
)

print("开始训练SVM模型...")
svm_model.fit(X_train_tfidf, y_train)
print("模型训练完成！")

